{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmYPAqKAY9UT"
   },
   "source": [
    "# Project 22: Activity Recognition\n",
    "## Authors: Alessandro Pomes, Simon Schmoll\n",
    "## Objectives: Classification of 7 activities which are tracked with a Single Chest-Mounted Accelerometer\n",
    "## What is done in the Notebook: The data is imported, processed and classified\n",
    "## As we followed a modular approach firstly the functions are defined which are later called for execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_bCZ-3XY9UU"
   },
   "source": [
    "## Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m65BwKPPY9UV"
   },
   "outputs": [],
   "source": [
    "# General Imports for more than one file\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# For reading the CSV\n",
    "from pandas import read_csv\n",
    "\n",
    "# Imports for the classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkyvAeiIY9UZ"
   },
   "source": [
    "# Importing of the dataset\n",
    "## Method definition for reading one of the available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h3k2nLsLY9Ua"
   },
   "outputs": [],
   "source": [
    "# Specifying engine = python because c engine can not handle 'sep'\n",
    "# @params: dataNum\n",
    "# @output: dataset (Dataframe)\n",
    "def read(data_num):\n",
    "    dataset = pd.read_csv('data/%d.csv' % (data_num), sep=',', header=None, engine='python', names=names_attributes)\n",
    "\n",
    "    # Comment in for printing out the array data and the size of the array\n",
    "    # print(array_data)\n",
    "    # print(np.size(array_data, 0))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE0K_A8zY9Ud"
   },
   "source": [
    "# Checking for missing data\n",
    "## In the following lines, we check for missing values as these can falsify our data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bvLMbgt8Y9Ue"
   },
   "outputs": [],
   "source": [
    "#First we transform the Dataframe into an numpy array\n",
    "#Specifying engine = python because c engine can not handle 'sep'\n",
    "# @params: dataNum\n",
    "# @output: dataset (Dataframe)\n",
    "def read(data_num):\n",
    "    names_attributes = ['sequentialNumber', 'xAcceleration', 'yAcceleration', 'zAcceleration', 'label']\n",
    "    dataset = read_csv('data/%d.csv' % (data_num), sep=',', header=None, engine='python', names=names_attributes)\n",
    "\n",
    "    # Comment in for printing out the array data and the size of the array\n",
    "    # print(array_data)\n",
    "    # print(np.size(array_data, 0))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# This function checks how balanced the data is\n",
    "# @Input: data_array with labels\n",
    "# @Output: list with different counts for the labels\n",
    "def count_labels(data_array):\n",
    "    label_data = data_array[:,[4]]\n",
    "    unique, counts = np.unique(label_data, return_counts=True)\n",
    "    ret = dict(zip(unique, counts))\n",
    "    return ret\n",
    "\n",
    "#3d plot Graph\n",
    "#Here we reate a simple funcion to create the plot of the data\n",
    "#of our original dataset. The different 7 colour corrispond our 7 labels\n",
    "#@input:database in a matrix Array\n",
    "#@output:plot of the point in a 3d structure\n",
    "\n",
    "def d3Plot(dataset):\n",
    "    def column(matrix, i):\n",
    "        return [row[i] for row in matrix]\n",
    "    ax = plt.axes(projection='3d')\n",
    "    # Data for three-dimensional scattered points\n",
    "    zdata= column(dataset,3)\n",
    "    ydata= column(dataset,2)\n",
    "    xdata= column(dataset,1)\n",
    "    lable= column(dataset,4)\n",
    "    colors = ['orange','green','blue','purple','yellow','black','orange','white']\n",
    "    ax.scatter3D(xdata, ydata, zdata, c=lable, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHzsW7P1Y9Uh"
   },
   "source": [
    "# Feature Engineering\n",
    "## The goal is to extract features from the preprocessed numpy array\n",
    "## But before we have to do a preprocessing step\n",
    "## 1. Step is to sequence the data in windows with 52 instances \n",
    "## Sidenote: it is of high importance to not mix two labels into the same window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c4DFDqhlY9Ui"
   },
   "outputs": [],
   "source": [
    "# For Feature Extraction we use a technique called window overlapping (Pierluigi Casale, Oriol Pujol, and Petia Radeva. Human activity recognition from accelerometer\n",
    "# data using a wearable device. Pattern Recognition and Image Analysis, pages 289–296, 2011). It has an overlap of 50% between the different\n",
    "# time series. As a time window 1 second is use --> corresponds to 52 samplings (52 Hz frequency)\n",
    "# Then we start with the sequencing\n",
    "# Slicing needs to be done as follows:\n",
    "# - it is not possible that 2 activities are grouped in one sequence (would falsify the outcome of the mean value)\n",
    "# - therefore only labels with the same value are grouped into one sequence\n",
    "# @params: array_data is list of array that contains the grouped data\n",
    "# @output: data_list which contains numpy arrays with the respective windows\n",
    "\n",
    "def grouping(array_data):\n",
    "    start = int(0)\n",
    "    end = int(52)\n",
    "    data_list = []\n",
    "    length = np.size(array_data, 0)\n",
    "    while start < length-52:\n",
    "        if(array_data[start][4] != array_data[end-1][4]):        # this control sequence is necessary to ensure that not two of the same\n",
    "            while(array_data[start][4] != array_data[end-1][4]): # labels are in one window\n",
    "                end = end -1\n",
    "            newArray = array_data[slice(start, end)]\n",
    "            start = end\n",
    "            end = end + 52\n",
    "        else:\n",
    "            newArray = array_data[slice(start, end)]\n",
    "            start = start + 26\n",
    "            end = end + 26\n",
    "        data_list.append(newArray)\n",
    "        if(end-52 > length - 1):\n",
    "            end = length-1\n",
    "\n",
    "    # Comment in to show the size and length of the data_list array\n",
    "    # print(np.size(data_list))\n",
    "    # print(len(data_list))\n",
    "    return data_list\n",
    "\n",
    "# This is an additional function which could be called to print a data list to a text file (e.g to examine it)\n",
    "# Comment in for printing the data to a text file\n",
    "# def sysout_to_text(dataList):\n",
    "#     file = open(\"tempFile\", \"w\")\n",
    "#     for item in dataList:\n",
    "#         file.write(\"%s\\n\" % item)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpDtWKPJY9Ul"
   },
   "source": [
    "# Feature Extraction\n",
    "# 2. Step we want to extract two feature types for each window (6 different features for each window - x-, y-, z- axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lyb0ffiAY9Um"
   },
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "\n",
    "# For Feature Extraction we use a technique called window overlapping (Pierluigi Casale, Oriol Pujol, and Petia Radeva. Human activity recognition from accelerometer\n",
    "# data using a wearable device. Pattern Recognition and Image Analysis, pages 289–296, 2011). It has an overlap of 50% between the different\n",
    "# time series. As a time window 1 second is use --> corresponds to 52 samplings (52 Hz frequency)\n",
    "# Then we start with the sequencing\n",
    "# Slicing needs to be done as follows:\n",
    "# - it is not possible that 2 activities are grouped in one sequence (would falsify the outcome of the mean value)\n",
    "# - therefore only labels with the same value are grouped into one sequence\n",
    "# @params: array_data is list of array that contains the grouped data\n",
    "# @output: data_list which contains numpy arrays with the respective windows\n",
    "\n",
    "def grouping(array_data):\n",
    "    start = int(0)\n",
    "    end = int(52)\n",
    "    data_list = []\n",
    "    length = np.size(array_data, 0)\n",
    "    while start < length-52:\n",
    "        if(array_data[start][4] != array_data[end-1][4]):        # this control sequence is necessary to ensure that not two of the same\n",
    "            while(array_data[start][4] != array_data[end-1][4]): # labels are in one window\n",
    "                end = end -1\n",
    "            newArray = array_data[slice(start, end)]\n",
    "            start = end\n",
    "            end = end + 52\n",
    "        else:\n",
    "            newArray = array_data[slice(start, end)]\n",
    "            start = start + 26\n",
    "            end = end + 26\n",
    "        data_list.append(newArray)\n",
    "        if(end-52 > length - 1):\n",
    "            end = length-1\n",
    "\n",
    "    # Comment in to show the size and length of the data_list array\n",
    "    # print(np.size(data_list))\n",
    "    # print(len(data_list))\n",
    "    return data_list\n",
    "\n",
    "# This is an additional function which could be called to print a data list to a text file (e.g to examine it)\n",
    "# Comment in for printing the data to a text file\n",
    "# def sysout_to_text(dataList):\n",
    "#     file = open(\"tempFile\", \"w\")\n",
    "#     for item in dataList:\n",
    "#         file.write(\"%s\\n\" % item)\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "#Now we need to get the mean value and standard deviation of all windows\n",
    "#@Params: grouped data_List containing the window arrays\n",
    "#@Output: mean value of x, y, z, standard deviation of the coordinates, target array\n",
    "def extract_features(data_list):\n",
    "    total_average_values = []\n",
    "    total_label = []\n",
    "    for row in data_list:\n",
    "        acceleration = np.nanmean(row, 0)\n",
    "        standard_deviation = np.std(row, 0)\n",
    "        temp_features = [acceleration[1], acceleration[2], acceleration[3], standard_deviation[1], standard_deviation[2], standard_deviation[3]]\n",
    "        label_array = [row[0][4]]\n",
    "        total_average_values.append(temp_features)\n",
    "        total_label.append(label_array)\n",
    "   # print(total_average_values)\n",
    "   # print(total_label)\n",
    "    #print(total_average_values)\n",
    "    feature = np.vstack(total_average_values)\n",
    "    target = np.vstack(total_label)\n",
    "\n",
    "    # comment in to print out lists\n",
    "    #print(feature)\n",
    "    #print(target)\n",
    "    return feature, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iigEpuZiY9Up"
   },
   "source": [
    "# Classification\n",
    "## The next step is the classification of the labels with the extracted features. Therefore the team used 2 different classifiers (Random Forest - as the main classifier build on the idea of the research paper by Pierluigi et al. {Pierluigi Casale, Oriol Pujol, and Petia Radeva. Human activity recognition from accelerometer data using a wearable device. Pattern Recognition and Image Analysis, pages 289–296, 2011} - and Support Vector machines as a comparison to the main classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TX-lg7NqY9Uq"
   },
   "outputs": [],
   "source": [
    "#function to classify without crossvalidaion:\n",
    "#we created here funtion to predict with Random forest and SVM the values with a simple division of the dataset.\n",
    "#we split infact the dataset in 2 part:Training and test set and we provide the follow estimators:\n",
    "#1)F1score\n",
    "#2)Accuracy\n",
    "#3)Confusion Matrix\n",
    "\n",
    "def classify(x_features, y_features):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_features, y_features.ravel(), test_size=0.2, random_state=0)\n",
    "    X_train.shape, y_train.shape\n",
    "    X_test.shape, y_test.shape\n",
    "    forest= RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "    model = forest.fit(X_train,y_train )\n",
    "    modelSv=clf.fit(X_train,y_train )\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    predicted_labelsSv = modelSv.predict(X_test)\n",
    "\n",
    "    # Compute the F1 score, also known as balanced F-score or F-measure\n",
    "    # The F1 score can be interpreted as a weighted average of the precision and recall,\n",
    "    #  where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    #  The relative contribution of precision and recall to the F1 score are equal.\n",
    "    #  The formula for the F1 score is:\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # In the multi-class and multi-label case, this is the weighted average of the F1 score of each class.\n",
    "\n",
    "    print(\" F1 score Random forest: %f\" % f1_score(y_test, predicted_labels, average='macro'))\n",
    "    print(\" F1 score precision with SV: %f\" % f1_score(y_test, predicted_labels, average='macro'))\n",
    "\n",
    "    print(\"Accuracy Random forest: %f\" % metrics.accuracy_score(y_test, predicted_labels))\n",
    "    print(\"Accuracy with SV: %f\" % metrics.accuracy_score(y_test, predicted_labelsSv))\n",
    "    #By definition a confusion matrix C is such that C_{i, j}\n",
    "    # is equal to the number of observations known to be in group i but predicted to be in group j.\n",
    "    print(\"Confusion Matrix Random forest: \")\n",
    "    print(confusion_matrix(y_test, predicted_labels, labels=[1, 2, 3, 4, 5, 6, 7]))\n",
    "    print(\"Confusion Matrix with SVM: \")\n",
    "    print(confusion_matrix(y_test, predicted_labelsSv, labels=[1, 2, 3, 4, 5, 6, 7]))\n",
    "    return\n",
    "\n",
    "#function to classify with crossvalidaion:\n",
    "#1)Accuracy\n",
    "#2)F1score\n",
    "\n",
    "\n",
    "def CrossValidation(x_features, y_features, kfold):\n",
    "    scoring = ['accuracy', 'f1_micro']\n",
    "    forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "    scoresSv = cross_validate(clf, x_features, y_features.ravel(), scoring=scoring, cv=kfold, return_train_score=False)\n",
    "    scores = cross_validate(forest, x_features, y_features.ravel(), scoring=scoring, cv=kfold, return_train_score=False)\n",
    "    print(\"Accuracy Random Forest: %0.2f (+/- %0.2f)\" % (scores['test_accuracy'].mean(), scores['test_accuracy'].std() * 2))\n",
    "    print(\"F1 Score Random Forest: %0.2f (+/- %0.2f)\" % (scores['test_f1_micro'].mean(), scores['test_f1_micro'].std() * 2))\n",
    "    print(\"Accuracy SVM: %0.2f (+/- %0.2f)\" % (scoresSv['test_accuracy'].mean() ,scoresSv['test_accuracy'].std() * 2))\n",
    "    print(\"F1 Score SVM: %0.2f (+/- %0.2f)\" % (scoresSv['test_f1_micro'].mean() ,scoresSv['test_f1_micro'].std() * 2))\n",
    "    print(\"Scores for the test folds (Random Forest)\",  scores['test_accuracy'])\n",
    "    print(\"Scores for the test folds (Support Vector Machine)\", scoresSv['test_accuracy'])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnL0SDPIY9Ut"
   },
   "source": [
    "# Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v5Oa9yVhY9Uu"
   },
   "outputs": [],
   "source": [
    "header= ['sequentialNumber','xAcceleration','yAcceleration','zAcceleration','label']\n",
    "\n",
    " # definition of the function for deleting rows with '0' as a label\n",
    " # @input: Pandas Dataframe, value of lable to delete\n",
    " # @output: array of int without lable 0\n",
    "def zeroDet( dataset, value ):\n",
    "    num = []\n",
    "    num=(dataset.loc[dataset['label'] == value].index.values)\n",
    "    dataClean=(dataset.drop(num))\n",
    "    dataClean = dataClean.to_numpy()\n",
    "    dataClean = dataClean.astype(np.int)\n",
    "    return dataClean\n",
    "\n",
    "#here we are trying to detect if there are some\n",
    "#missing data in all dataset.\n",
    "#To prove there aren't missing data,\n",
    "#output should generate empty arrays\n",
    "def MisData():\n",
    "    num = 1\n",
    "    while(num < 16):\n",
    "        dataset = read_csv('data/%d.csv' % (num) , names=header)\n",
    "        boolData = dataset.isnull()\n",
    "        #for name in header:\n",
    "            #print(boolData.loc[boolData[name] == True])\n",
    "        num += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0cHTsOoKY9Ux",
    "outputId": "4d992683-cdab-4022-ad17-8c63a180e502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of every label, starting by one to seven {1: 33677, 2: 928, 3: 11179, 4: 26860, 5: 3191, 6: 2917, 7: 83748}\n",
      "(array([1295,   35,  429, 1033,  122,  112, 3220]), array([1.        , 1.85714286, 2.71428571, 3.57142857, 4.42857143,\n",
      "       5.28571429, 6.14285714, 7.        ]))\n",
      " F1 score Random forest: 0.705654\n",
      " F1 score precision with SV: 0.705654\n",
      "Accuracy Random forest: 0.923200\n",
      "Accuracy with SV: 0.868800\n",
      "Confusion Matrix Random forest: \n",
      "[[242   0   0   2   0   0   7]\n",
      " [  0   1   0   4   0   0   0]\n",
      " [  0   0  44   6   0   0  39]\n",
      " [  0   0   0 199   2   1   1]\n",
      " [  0   0   0   1  17   1   3]\n",
      " [  0   0   0  22   0   5   1]\n",
      " [  2   0   3   1   0   0 646]]\n",
      "Confusion Matrix with SVM: \n",
      "[[243   0   0   1   0   1   6]\n",
      " [  0   0   0   3   1   0   1]\n",
      " [  0   0   1   6   0   0  82]\n",
      " [  0   0   2 190   7   1   3]\n",
      " [  0   0   0   2  18   0   2]\n",
      " [  0   0   0  24   0   2   2]\n",
      " [  9   1   6   3   1   0 632]]\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "dataframe = read(1)\n",
    "#print(dataframe)\n",
    "# Delete incorrect Data\n",
    "\n",
    "cleaned_data = zeroDet(dataframe, 0)\n",
    "\n",
    "#Check how balanced data is\n",
    "counts = count_labels(cleaned_data)\n",
    "print(\"Instances of every label, starting by one to seven\",counts) \n",
    "\n",
    "\n",
    "#Feature Extraction\n",
    "grouped_data = grouping(cleaned_data)\n",
    "features = extract_features(grouped_data)\n",
    "\n",
    "print(np.histogram(features.__getitem__(1),7))\n",
    "\n",
    "\n",
    "#we are calling the function for classify our data, first with a simple\n",
    "#splitting of the dataset to divide test and training set, after\n",
    "#using k-crossvalidation with two different train model: 1)RandomForest classifier , 2)Super Vector Machine\n",
    "classify(features.__getitem__(0),features.__getitem__(1))\n",
    "#CrossValidation(features.__getitem__(0),features.__getitem__(1), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Uv386TY9U2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "P22 Activity Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
