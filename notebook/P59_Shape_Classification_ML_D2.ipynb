{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGg1vOAhXHOn"
   },
   "source": [
    "# Project 59 Shape Classification\n",
    "\n",
    "## Authors: Julen Etxaniz and Ibon Urbina\n",
    "\n",
    "## Objectives: The goal of the project is to compare different classification algorithms on the solution of one or more shape datasets. \n",
    "\n",
    "## What is done in the Notebook: \n",
    "### Importing the libraries\n",
    "### Reading the datasets\n",
    "### Processing the dataset\n",
    "### Preparing data for classification\n",
    "### Dividing dataset in train and test sets for validation\n",
    "### Defining the classifiers\n",
    "### Learning the classifiers\n",
    "### Using the classifier for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG9Sa_dtXHOo"
   },
   "source": [
    "# Importing the libraries\n",
    " We start by importing all relevant libraries to be used in the notebook.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Yyc03UBFXHOp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_51C9mUlXHO2"
   },
   "source": [
    "# Reading the datasets\n",
    "We read the plane and car datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_51C9mUlXHO2"
   },
   "source": [
    "## Reading the plane dataset\n",
    "We read the 210 files that contain the instances of the plane classification problem.\n",
    "\n",
    "We concatenate all the instances in a unique dataframe called \"plane_mats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zo31q5cwXHO3",
    "outputId": "a8dc5a71-aacc-4a23-a48f-a49ad491e7be"
   },
   "outputs": [],
   "source": [
    "plane_dir = \"../shape_dataset/plane_data/\"\n",
    "plane_mats = []\n",
    "for file in os.listdir(plane_dir) :\n",
    "    plane_mats.append(loadmat(plane_dir + file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WykGUAkPXHPE"
   },
   "source": [
    "We check the dataset is correct, looking at the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "b3Wt7DtuXHPG",
    "outputId": "d1b01efa-953d-4eac-8123-0c121df1f490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in the plane dataset is 210\n"
     ]
    }
   ],
   "source": [
    "print('The number of samples in the plane dataset is', len(plane_mats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_51C9mUlXHO2"
   },
   "source": [
    "## Reading the car dataset\n",
    "We read the 120 files that contain the instances of the car classification problem.\n",
    "\n",
    "We concatenate all the instances in a unique dataframe called \"car_mats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zo31q5cwXHO3",
    "outputId": "a8dc5a71-aacc-4a23-a48f-a49ad491e7be",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "car_dir = \"../shape_dataset/car_data/\"\n",
    "car_mats = []\n",
    "for file in os.listdir(car_dir) :\n",
    "    car_mats.append(loadmat(car_dir + file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WykGUAkPXHPE"
   },
   "source": [
    "We check the dataset is correct, looking at the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b3Wt7DtuXHPG",
    "outputId": "d1b01efa-953d-4eac-8123-0c121df1f490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in the car dataset is 120\n"
     ]
    }
   ],
   "source": [
    "print('The number of samples in the car dataset is', len(car_mats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MJ8lJoYXHPL"
   },
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "In this problem there are four classes that correspond to the types of vehicles: 'van' 'saab' 'bus' 'opel'. \n",
    "For using the classifiers, we need to convert each of these strings in the dataset to a number between 1 and 4. \n",
    "That is what we do in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atXEc2AGXHPM",
    "outputId": "6ccb6362-b469-47a1-b9dd-16012ac37516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['van' 'saab' 'bus' 'opel']\n"
     ]
    }
   ],
   "source": [
    "# The unique values in the column 'VEHICLE' of the dataset.\n",
    "Vehicle_Types = all_tables['VEHICLE'].unique()\n",
    "print(Vehicle_Types)\n",
    "\n",
    "# We make a copyt of the dataset\n",
    "my_dataset = all_tables.copy()\n",
    "\n",
    "# A Map between the four names and the four numbers is created\n",
    "map_to_int = {name: n for n, name in enumerate(Vehicle_Types)}\n",
    "\n",
    "# A column is created in the new dataset where words are replaced by numbers\n",
    "my_dataset['CLASS'] = my_dataset['VEHICLE'].replace(map_to_int)\n",
    "\n",
    "# Finally we delete the column with the names from the new dataset\n",
    "my_dataset = my_dataset.drop('VEHICLE',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0X42f8vXHPQ"
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#print(np.histogram(my_dataset['CLASS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfvD3kjXHPV"
   },
   "source": [
    "# Preparing data for classification\n",
    "\n",
    "To apply the classifiers, we need to separate in two different sets the features and the classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3WhjQgpXHPV"
   },
   "outputs": [],
   "source": [
    "# The names of the features are the first 18 attributes\n",
    "features_names = names_attributes[1:18]\n",
    "\n",
    "# all_features contains the features for the 846 samples.\n",
    "all_features = my_dataset[features_names]\n",
    "\n",
    "# all_theclass contains the classes (values between 1 and 4) for the 846 samples \n",
    "all_theclass = my_dataset[\"CLASS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW2l7-cLXHPc"
   },
   "source": [
    "# Dividing dataset in train and test  sets for validation\n",
    "\n",
    "Also, to evaluate the accuracy of the classifiers in the dataset we will split the data in two sets. Train and Test data. \n",
    "Each set will have the same number of samples  (846/2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-7tCtJnXHPe"
   },
   "outputs": [],
   "source": [
    "# We divide the data into two sets (train and test)\n",
    "\n",
    "# Number of samples in the train and test sets (half of the number of samples)\n",
    "n_samples = int(len(all_features)/2)\n",
    "\n",
    "# The train data are the first half of all_features\n",
    "train_data = all_features[:n_samples]\n",
    "train_class = all_theclass[:n_samples]\n",
    "\n",
    "# The test data are the second half of all_features\n",
    "test_data = all_features[n_samples:]\n",
    "test_class = all_theclass[n_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3bAcwEkXHPk"
   },
   "source": [
    "# Defining the classifiers\n",
    "We define the three classifiers used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ0-wN-9XHPn"
   },
   "outputs": [],
   "source": [
    "dt  = DecisionTreeClassifier()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lg  = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6v3DiLkXHPv"
   },
   "source": [
    "# Learning the classifiers\n",
    "We used the train data to learn the three classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JpdAuf5XHPw",
    "outputId": "65c3ac39-986f-41f0-c1c2-9d0570b3755a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(train_data,train_class)\n",
    "lda.fit(train_data,train_class)\n",
    "lg.fit(train_data,train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae1aQRdbXHP3"
   },
   "source": [
    "# Using the classifier for predictions\n",
    "We predict the class of the samples in the test data with the three classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRf1R87PXHP3"
   },
   "outputs": [],
   "source": [
    "dt_test_predictions = dt.predict(test_data)\n",
    "lda_test_predictions = lda.predict(test_data)\n",
    "lg_test_predictions = lg.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBu8ruEFXHP6"
   },
   "source": [
    "# Computing the accuracy\n",
    "\n",
    "Finally, we compute the accuracy using the three classifiers and print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qC_IykgXHP7",
    "outputId": "9f81b405-6f87-4805-cc72-e31592f3a9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the decision tree : 0.633569739953\n",
      "Accuracy for LDA : 0.706855791962\n",
      "Accuracy for logistic regression: 0.737588652482\n"
     ]
    }
   ],
   "source": [
    "dt_acc =  accuracy_score(test_class,dt_test_predictions)\n",
    "lda_acc =  accuracy_score(test_class,lda_test_predictions)\n",
    "lg_acc =  accuracy_score(test_class,lg_test_predictions)\n",
    "print(\"Accuracy for the decision tree :\",dt_acc)\n",
    "print(\"Accuracy for LDA :\",lda_acc)\n",
    "print(\"Accuracy for logistic regression:\",lg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE1QfhVCXHP_"
   },
   "source": [
    "# Computing the confusion matrices\n",
    "Finally we compute the confusion matrices for the three classifiers. We print the confusion matrices and also generate the latex code to insert it in our written report. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X5o8OuCXHQA",
    "outputId": "c5d4b93a-7437-460c-f964-3b63388799df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix decision tree\n",
      "col_0   0   1   2   3\n",
      "CLASS                \n",
      "0      74   8   5   7\n",
      "1       6  50   8  54\n",
      "2       0   6  87   4\n",
      "3       8  40   9  57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrr}\\n\\\\toprule\\ncol\\\\_0 &   0 &   1 &   2 &   3 \\\\\\\\\\nCLASS &     &     &     &     \\\\\\\\\\n\\\\midrule\\n0     &  74 &   8 &   5 &   7 \\\\\\\\\\n1     &   6 &  50 &   8 &  54 \\\\\\\\\\n2     &   0 &   6 &  87 &   4 \\\\\\\\\\n3     &   8 &  40 &   9 &  57 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion matrix decision tree\")\n",
    "cm_dt = pd.crosstab(test_class,dt_test_predictions)\n",
    "print(cm_dt)\n",
    "cm_dt.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d2ry-lBXHQD",
    "outputId": "e6e54a8a-229c-4919-ef7d-bc03f42edfab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix LDA\n",
      "col_0   0   1   2   3\n",
      "CLASS                \n",
      "0      80   6   3   5\n",
      "1       4  65   3  46\n",
      "2       1   2  94   0\n",
      "3       7  45   2  60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrr}\\n\\\\toprule\\ncol\\\\_0 &   0 &   1 &   2 &   3 \\\\\\\\\\nCLASS &     &     &     &     \\\\\\\\\\n\\\\midrule\\n0     &  80 &   6 &   3 &   5 \\\\\\\\\\n1     &   4 &  65 &   3 &  46 \\\\\\\\\\n2     &   1 &   2 &  94 &   0 \\\\\\\\\\n3     &   7 &  45 &   2 &  60 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion matrix LDA\")\n",
    "cm_lda = pd.crosstab(test_class,lda_test_predictions)\n",
    "print(cm_lda)\n",
    "cm_lda.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxv3IrD8XHQL",
    "outputId": "1babf92f-2844-4c60-9323-64433a8afd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Logistic regression\n",
      "col_0   0   1   2   3\n",
      "CLASS                \n",
      "0      83   3   2   6\n",
      "1       3  74   3  38\n",
      "2       1   3  93   0\n",
      "3       5  46   1  62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrr}\\n\\\\toprule\\ncol\\\\_0 &   0 &   1 &   2 &   3 \\\\\\\\\\nCLASS &     &     &     &     \\\\\\\\\\n\\\\midrule\\n0     &  83 &   3 &   2 &   6 \\\\\\\\\\n1     &   3 &  74 &   3 &  38 \\\\\\\\\\n2     &   1 &   3 &  93 &   0 \\\\\\\\\\n3     &   5 &  46 &   1 &  62 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion matrix Logistic regression\")\n",
    "cm_lg = pd.crosstab(test_class,lg_test_predictions)\n",
    "print(cm_lg)\n",
    "cm_lg.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YjHfCbrXHQS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Example_Notebook_Course_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
